# -*- coding: utf-8 -*-
"""IMDB_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FTbhrqiZC5r6FTmkS5_6e5GGlYqwxWAO
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import string
from nltk.corpus import stopwords

from google.colab import drive
drive.mount('/content/drive')

path='/content/drive/MyDrive/IMDB_Dataset.csv'
data=pd.read_csv(path)

data.shape

data_IMDB=data.iloc[:30000] # Because dataset is very big
data_IMDB['review'][1]
data_IMDB.shape

data_IMDB['sentiment'].value_counts()

"""## Preprocessing of data"""

data_IMDB.isnull().sum()

data_IMDB.duplicated().sum()

data_IMDB.drop_duplicates(inplace=True)

data_IMDB.duplicated().sum()

data_IMDB.shape

import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
import string

# pipeline for the text preprocessing
def preprocess_text(text):
    # Tokenization of the text
    tokens = nltk.word_tokenize(text)
    # Removing stopwords
    stopwords_list = stopwords.words('english')
    filtered_tokens = [token for token in tokens if token not in stopwords_list]
    
    # converting tokens into the lower formats
    lowercase_tokens = [token.lower() for token in filtered_tokens]
    
    # Removing punctuation
    translator = str.maketrans('', '', string.punctuation)
    no_punct_tokens = [token.translate(translator) for token in lowercase_tokens]
    
    # Removing empty tokens from the data
    final_tokens = [token for token in no_punct_tokens if token]
    
    # Joining tokens back
    preprocessed_text = ' '.join(final_tokens)
    
    return preprocessed_text

data_IMDB['review']= data_IMDB['review'].apply(preprocess_text)

X= data_IMDB['review']
print(X.shape)
y = data_IMDB['sentiment']

print(X)

print(y)

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
y = encoder.fit_transform(y)

print(y)

"""## LSTM Model"""

import tensorflow 
import keras
from tensorflow.keras.preprocessing.text import one_hot
vocab_size=5000
review= [one_hot(words,vocab_size) for words in data_IMDB['review']]
review

from tensorflow.keras.preprocessing import sequence
maxlen=100
padded_review = sequence.pad_sequences(review, maxlen=maxlen)

from sklearn.model_selection import train_test_split

# Split the data into train and test sets
x_train, x_test, y_train, y_test = train_test_split(padded_review, y, test_size=0.2, random_state=0)

# Split the train set into train and validation sets
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=0)

"""## LSTM with one layer

## 4(b)
"""

import numpy as np
from keras.datasets import imdb
from keras.models import Sequential
from keras.layers import LSTM, Dense, Embedding
from keras.preprocessing import sequence
from keras.callbacks import EarlyStopping

# set parameters
max_features = 6000 
maxlen = 100 
batch_size = 32 
epochs = 10 

# define the model1
model1 = Sequential()
model1.add(Embedding(max_features, 32, input_length=maxlen))
model1.add(LSTM(64))
model1.add(Dense(1, activation='sigmoid'))

# compile the model1
model1.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
# define early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)
# train the model1
hist= model1.fit(np.expand_dims(x_train, axis=2), y_train,
          batch_size=batch_size,
          epochs=epochs,
          validation_data=(np.expand_dims(x_val, axis=2), y_val), callbacks=[early_stop])

# evaluate the model1 on the testing data
y_pred_prob = model1.predict(x_test)
y_pred = (y_pred_prob > 0.5).astype(int)
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
acc = accuracy_score(y_test, y_pred)
print("Test Accuracy: ", acc)
# precision score
precision = precision_score(y_test, y_pred)
print('Precision:', precision)

# recall score
recall = recall_score(y_test, y_pred)
print('Recall:', recall)

# F1 score
f1 = f1_score(y_test, y_pred)
print('F1 score:', f1)

# confusion matrix
confusion_mat = confusion_matrix(y_test, y_pred)
print('Confusion matrix:\n', confusion_mat)

import matplotlib.pyplot as plt

# plot accuracy during training
plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""## LSTM with 2 layers

## 4(b)
"""

import tensorflow as tf

# define hyperparameters
vocab_size = 6000
embedding_dim = 32
hidden_dim = 64
num_epochs = 10
batch_size = 32

# define the model
model2= tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim),
    tf.keras.layers.LSTM(hidden_dim, return_sequences=True),
    tf.keras.layers.LSTM(hidden_dim),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# compile the model
model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# define early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)
hist= model2.fit(np.expand_dims(x_train, axis=2), y_train,
          batch_size=batch_size,
          epochs=epochs,
          validation_data=(np.expand_dims(x_val, axis=2), y_val),callbacks=[early_stop])

# evaluate the model2 on the testing data
y_pred_prob = model2.predict(x_test)
y_pred = (y_pred_prob > 0.5).astype(int)

acc = accuracy_score(y_test, y_pred)
print("Test Accuracy: ", acc)
# precision score
precision = precision_score(y_test, y_pred)
print('Precision:', precision)

# recall score
recall = recall_score(y_test, y_pred)
print('Recall:', recall)

# F1 score
f1 = f1_score(y_test, y_pred)
print('F1 score:', f1)

# confusion matrix
confusion_mat = confusion_matrix(y_test, y_pred)
print('Confusion matrix:\n', confusion_mat)

import matplotlib.pyplot as plt

# plot accuracy during training
plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()