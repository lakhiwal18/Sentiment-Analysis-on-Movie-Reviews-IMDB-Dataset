# -*- coding: utf-8 -*-
"""IMDB_RNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19duJmA-94eZfJZ77HpD5mD92cpsMPkje

## QUES_4(a)
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import string
from nltk.corpus import stopwords

from google.colab import drive
drive.mount('/content/drive')

path='/content/drive/MyDrive/IMDB_Dataset.csv'
data=pd.read_csv(path)

data.shape

data_IMDB=data
data_IMDB = data.iloc[:30000]
data_IMDB['review'][1]
data_IMDB.shape

data_IMDB['sentiment'].value_counts()

"""## Preprocessing of data"""

data_IMDB.isnull().sum()

# Removing the duplicate values
data_IMDB.drop_duplicates(inplace=True)

data_IMDB.shape

import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
import string

#Pipeline for the text preprocessing 
def preprocessing_text(text):
    # Tokenization of the text
    tokens = nltk.word_tokenize(text)
    # Removing stopwords
    stopwords_list = stopwords.words('english')
    filtered_tokens = [token for token in tokens if token not in stopwords_list]
    
    # converting tokens in the lowercase format
    lowercase_tokens = [token.lower() for token in filtered_tokens]
    
    # Removing puntuation
    translator = str.maketrans('', '', string.punctuation)
    no_punct_tokens = [token.translate(translator) for token in lowercase_tokens]
    
    # Removing empty tokens from the data
    final_tokens = [token for token in no_punct_tokens if token]
    
    # Joining tokens back
    preprocessed_text = ' '.join(final_tokens)
    
    return preprocessed_text

data_IMDB['review']= data_IMDB['review'].apply(preprocessing_text)

data_IMDB.duplicated().sum()

X= data_IMDB['review']
print(X.shape)
y = data_IMDB['sentiment']

print(X)

print(y)

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

print(y)

"""##RNN Model"""

from tensorflow.keras.preprocessing.text import one_hot
VOCAB_SIZE = 5000
reviews = [one_hot(words, VOCAB_SIZE) for words in data_IMDB['review']]

from tensorflow.keras.preprocessing import sequence
maxlen=100
padded_review = sequence.pad_sequences(reviews, maxlen=maxlen)

from sklearn.model_selection import train_test_split

# Splits the data into train and test sets
x_train, x_test, y_train, y_test = train_test_split(padded_review, y, test_size=0.3, random_state=0)

# Splits the train set into train and validation sets
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=0)

from keras.models import Sequential
from keras.layers import Dense, SimpleRNN, Embedding
from sklearn.metrics import accuracy_score, confusion_matrix
from keras.callbacks import EarlyStopping
vocab_size = 6000 
embedding_dim = 32
from keras.layers import Dropout

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen))
model.add(SimpleRNN(64))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# define early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)

# fit the model on the training data with early stopping
history = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_val, y_val), callbacks=[early_stop])

# evaluate the model on the testing data
y_pred_prob = model.predict(x_test)
y_pred = (y_pred_prob > 0.5).astype(int)
acc = accuracy_score(y_test, y_pred)
print("Test Accuracy: ", acc)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# accuracy score
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)

# precision score
precision = precision_score(y_test, y_pred)
print('Precision:', precision)

# recall score
recall = recall_score(y_test, y_pred)
print('Recall:', recall)

# F1 score
f1 = f1_score(y_test, y_pred)
print('F1 score:', f1)

# confusion matrix
confusion_mat = confusion_matrix(y_test, y_pred)
print('Confusion matrix:\n', confusion_mat)

import matplotlib.pyplot as plt

#history = model.fit(x_train, y_train, epochs=15, batch_size=64, validation_data=(x_val, y_val))

# plot training and validation loss curves
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss Curves')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import matplotlib.pyplot as plt

# plot accuracy during training
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

from keras.models import Sequential
from keras.layers import Dense, SimpleRNN, Embedding
from sklearn.metrics import accuracy_score, confusion_matrix
from keras.callbacks import EarlyStopping
vocab_size = 6000 
embedding_dim = 32
from keras.layers import Dropout

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen))
model.add(SimpleRNN(256))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# define early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)

# fit the model on the training data with early stopping
history = model.fit(x_train, y_train, epochs=15, batch_size=64, validation_data=(x_val, y_val), callbacks=[early_stop])

# evaluate the model on the testing data
y_pred_prob = model.predict(x_test)
y_pred = (y_pred_prob > 0.5).astype(int)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
acc = accuracy_score(y_test, y_pred)
print("Test Accuracy: ", acc)
# precision score
precision = precision_score(y_test, y_pred)
print('Precision:', precision)

# recall score
recall = recall_score(y_test, y_pred)
print('Recall:', recall)

# F1 score
f1 = f1_score(y_test, y_pred)
print('F1 score:', f1)

# confusion matrix
confusion_mat = confusion_matrix(y_test, y_pred)
print('Confusion matrix:\n', confusion_mat)

import matplotlib.pyplot as plt

#history = model.fit(x_train, y_train, epochs=15, batch_size=64, validation_data=(x_val, y_val))

# plot training and validation loss curves
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss Curves')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import matplotlib.pyplot as plt

# plot accuracy during training
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()