# -*- coding: utf-8 -*-
"""IMDB_Bidirectional_glove_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17h99_WBEepvarunfKmOCatBb8WaqgSWm
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import string
from nltk.corpus import stopwords

from google.colab import drive
drive.mount('/content/drive')

path='/content/drive/MyDrive/IMDB_Dataset.csv'
data=pd.read_csv(path)

data.shape

data_IMDB=data.iloc[:30000]
# data_IMDB = data.iloc[:10000]
data_IMDB['review'][1]
data_IMDB.shape

data_IMDB['sentiment'].value_counts()

"""## Preprocessing of data"""

data_IMDB.isnull().sum()

data_IMDB.duplicated().sum()

data_IMDB.drop_duplicates(inplace=True)

data_IMDB.duplicated().sum()

data_IMDB.shape

import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
import string

# Define the text preprocessing pipeline
def preprocess_text(text):
    # Tokenize the text
    tokens = nltk.word_tokenize(text)
    # Remove stopwords
    stopwords_list = stopwords.words('english')
    filtered_tokens = [token for token in tokens if token not in stopwords_list]
    
    # Lowercase the tokens
    lowercase_tokens = [token.lower() for token in filtered_tokens]
    
    # Remove punctuation
    translator = str.maketrans('', '', string.punctuation)
    no_punct_tokens = [token.translate(translator) for token in lowercase_tokens]
    
    # Remove empty tokens
    final_tokens = [token for token in no_punct_tokens if token]
    
    # Join the tokens back into a single string
    preprocessed_text = ' '.join(final_tokens)
    
    return preprocessed_text

data_IMDB['review']= data_IMDB['review'].apply(preprocess_text)

X= data_IMDB['review']
print(X.shape)
y = data_IMDB['sentiment']

print(X)

print(y)

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
y = encoder.fit_transform(y)

print(y)

"""##RNN Model"""

!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip glove*.zip

# 3. Tokenize
import numpy as np
import keras.backend as K
from keras.utils import to_categorical
from keras.preprocessing.text import Tokenizer
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional
from keras.callbacks import EarlyStopping

tokenizer = Tokenizer()
tokenizer.fit_on_texts(data_IMDB['review'])
sequences = tokenizer.texts_to_sequences(data_IMDB['review'])

word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

MAX_SEQUENCE_LENGTH=100
from tensorflow.keras.preprocessing import sequence
maxlen=100
data = sequence.pad_sequences(sequences,maxlen=maxlen)
print(data.shape)

# print(labels)

labels = np.array(y)
print('Shape of data tensor:', data.shape)
print('Shape of label tensor:', labels.shape)

from sklearn.model_selection import train_test_split

# Split the data into train and test sets
x_train, x_test, y_train, y_test = train_test_split(data, y, test_size=0.4, random_state=0)

# Split the train set into train and validation sets
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=0)


print('Number of samples in training set:', len(x_train))
print('Number of samples in validation set:', len(x_val))

# Load pre-trained word embeddings (GloVe)
my_embeddings = {}
with open('/content/glove.6B.50d.txt', 'r', encoding='utf-8') as f:
    for text in f:
        values = text.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        my_embeddings[word] = coefs


print('Found %s word vectors.' % len(my_embeddings))

EMBEDDING_DIM = 50
embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))
for word, index in word_index.items():
    word_embedding = my_embeddings.get(word)
    if word_embedding is not None:
        embedding_matrix[index] = word_embedding


from keras.layers import Embedding

embedding_layer = Embedding(len(word_index) + 1,
                            EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=MAX_SEQUENCE_LENGTH,
                            trainable=False)
def recall(y_true, y_pred):
    """
    Calculates the recall metric.

    Args:
        y_true: array-like of shape (n_samples,) containing true labels.
        y_pred: array-like of shape (n_samples,) containing predicted labels.

    Returns:
        Recall: float, the calculated recall metric.
    """
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())

    return recall


def precision(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())

    return precision



# Training the LSTM model

batch_size = 64

model = Sequential()

model.add(embedding_layer)

model.add(Bidirectional(LSTM(64)))

model.add(Dropout(0.20))

model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy',precision,recall])
# define early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)

print('Train...')

hist= model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=16,
          validation_data=[x_val, y_val],callbacks=[early_stop])

loss,acc,precision,recall = model.evaluate(x_test, y_test)
print("Loss: ", loss)
print("Accuracy: ", acc)
print("Precision: ", precision)
print("Recall: ", recall)

from sklearn.metrics import confusion_matrix, f1_score

# Make predictions on test data
y_pred = (model.predict(x_test) > 0.5).astype(int)


# Calculate confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# Calculate F1 score
f1 = f1_score(y_test, y_pred)
print("F1 Score:", f1)

import matplotlib.pyplot as plt

#hist = model.fit(x_train, y_train, epochs=15, batch_size=64, validation_data=(x_val, y_val))

# plot training and validation loss curves
plt.plot(hist.history['loss'], label='Training Loss')
plt.plot(hist.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss Curves')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()